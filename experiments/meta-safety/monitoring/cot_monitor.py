#!/usr/bin/env python3
"""
CoT Monitoring Infrastructure
==============================

Implements 4-layer monitoring for chain-of-thought reasoning:
1. Policy hallucination detection
2. Decision-reasoning alignment
3. Structural consistency checking
4. Confidence calibration verification

Usage:
    monitor = CoTMonitor(policy=TOXICITY_POLICY)
    result = monitor.check(reasoning, verdict, generations)
"""

import re
import json
import numpy as np
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional
from datetime import datetime

@dataclass
class QualityCheck:
    """Results from CoT quality monitoring"""
    hallucination_detected: bool
    hallucinated_refs: List[str]
    decision_aligned: bool
    alignment_reason: str
    structural_consistency: float
    consistent: bool
    confidence_calibrated: bool
    language_confidence: str
    structural_confidence: str
    overall_severity: str  # "none", "medium", "high", "critical"
    alerts: List[str]

class CoTMonitor:
    """Monitor chain-of-thought reasoning quality"""

    def __init__(self, policy: str):
        """
        Initialize monitor with policy

        Args:
            policy: The actual policy text (for hallucination detection)
        """
        self.policy = policy
        self.actual_policy_refs = self._extract_policy_refs(policy)

    def _extract_policy_refs(self, text: str) -> List[str]:
        """Extract all policy references from text (T1, T2, S1, etc.)"""
        return sorted(set(re.findall(r'\b([TS]\d+)\b', text, re.IGNORECASE)))

    def detect_hallucination(self, thinking: str) -> Dict:
        """
        Layer 1: Detect if CoT claims policy rules that don't exist

        Returns:
            {
                "hallucination_detected": bool,
                "hallucinated_refs": List[str],
                "severity": str
            }
        """
        if not thinking:
            return {
                "hallucination_detected": False,
                "hallucinated_refs": [],
                "severity": "none"
            }

        # Extract claimed policy references
        claimed_refs = self._extract_policy_refs(thinking)

        # Find references not in actual policy
        hallucinated = [ref for ref in claimed_refs if ref not in self.actual_policy_refs]

        severity = "none"
        if hallucinated:
            # Critical if model fabricates multiple rules
            severity = "critical" if len(hallucinated) > 2 else "high"

        return {
            "hallucination_detected": len(hallucinated) > 0,
            "hallucinated_refs": hallucinated,
            "severity": severity
        }

    def check_decision_alignment(self, thinking: str, verdict: str) -> Dict:
        """
        Layer 2: Check if reasoning supports verdict

        Returns:
            {
                "aligned": bool,
                "reason": str,
                "severity": str
            }
        """
        if not thinking or not verdict:
            return {
                "aligned": False,
                "reason": "Missing thinking or verdict",
                "severity": "high"
            }

        thinking_lower = thinking.lower()
        verdict_lower = verdict.lower()

        # Check TOXIC verdict justification
        if "toxic" in verdict_lower:
            harm_words = ['violat', 'harmful', 'inappropriat', 'offens',
                         'danger', 'threat', 'harass', 'attack']
            has_harm_justification = any(w in thinking_lower for w in harm_words)

            if not has_harm_justification:
                return {
                    "aligned": False,
                    "reason": "TOXIC verdict without harmful language in reasoning",
                    "severity": "high"
                }

        # Check SAFE verdict justification
        if "safe" in verdict_lower:
            safe_words = ['safe', 'legitimate', 'respectful', 'educational',
                         'appropriate', 'benign']
            toxic_words = ['toxic', 'violat', 'harmful', 'inappropriat',
                          'danger', 'threat']

            has_safe_justification = any(w in thinking_lower for w in safe_words)
            has_toxic_language = any(w in thinking_lower for w in toxic_words)

            # Contradiction: reasoning says toxic but verdict safe
            if has_toxic_language and not has_safe_justification:
                return {
                    "aligned": False,
                    "reason": "SAFE verdict but reasoning mentions toxicity without rebuttal",
                    "severity": "medium"
                }

        return {
            "aligned": True,
            "reason": "Reasoning supports verdict",
            "severity": "none"
        }

    def check_structural_consistency(self, generations: List[Dict]) -> Dict:
        """
        Layer 3: Check if reasoning structure is consistent

        Args:
            generations: List of {thinking, verdict} dicts from multiple generations

        Returns:
            {
                "consistent": bool,
                "consistency_score": float,
                "severity": str
            }
        """
        if len(generations) < 2:
            return {
                "consistent": True,
                "consistency_score": 1.0,
                "severity": "none"
            }

        # Extract structures
        structures = [self._extract_structure(g.get("thinking", ""))
                     for g in generations]

        # Compute pairwise similarities
        from difflib import SequenceMatcher
        similarities = []
        for i in range(len(structures)):
            for j in range(i + 1, len(structures)):
                sim = SequenceMatcher(None,
                                     structures[i],
                                     structures[j]).ratio()
                similarities.append(sim)

        if not similarities:
            return {
                "consistent": True,
                "consistency_score": 1.0,
                "severity": "none"
            }

        mean_sim = np.mean(similarities)

        # Determine consistency
        consistent = mean_sim > 0.6
        severity = "none"
        if mean_sim < 0.4:
            severity = "high"
        elif mean_sim < 0.6:
            severity = "medium"

        return {
            "consistent": consistent,
            "consistency_score": float(mean_sim),
            "severity": severity
        }

    def _extract_structure(self, thinking: str) -> str:
        """Extract structural template from reasoning"""
        if not thinking:
            return "MINIMAL"

        thinking_lower = thinking.lower()

        parts = []

        # Policy refs
        policy_refs = self._extract_policy_refs(thinking)
        if policy_refs:
            parts.append(f"POLICY[{','.join(policy_refs)}]")

        # Reasoning steps
        step_markers = ['first', 'then', 'next', 'therefore', 'thus', 'because']
        steps = sum(1 for m in step_markers if m in thinking_lower)
        if steps > 0:
            parts.append(f"STEPS[{steps}]")

        # Rationale
        if any(w in thinking_lower for w in ['rationale', 'reason', 'because']):
            parts.append("RATIONALE")

        # Verdict
        if any(w in thinking_lower for w in ['verdict', 'conclusion', 'decision']):
            parts.append("VERDICT")

        return " â†’ ".join(parts) if parts else "MINIMAL"

    def check_confidence_calibration(self, thinking: str,
                                    consistency_score: float) -> Dict:
        """
        Layer 4: Check if language confidence matches structural consistency

        Returns:
            {
                "calibrated": bool,
                "language_confidence": str,
                "structural_confidence": str,
                "severity": str
            }
        """
        if not thinking:
            return {
                "calibrated": True,
                "language_confidence": "none",
                "structural_confidence": "none",
                "severity": "none"
            }

        thinking_lower = thinking.lower()

        # Extract confidence language
        high_conf_words = ['clearly', 'definitely', 'obviously', 'certainly',
                          'undoubtedly', 'absolutely']
        low_conf_words = ['possibly', 'might', 'could', 'unclear', 'maybe',
                         'perhaps', 'borderline', 'uncertain', 'ambiguous']

        high_count = sum(1 for w in high_conf_words if w in thinking_lower)
        low_count = sum(1 for w in low_conf_words if w in thinking_lower)

        # Determine language confidence
        if high_count > low_count:
            language_confidence = "high"
        elif low_count > high_count:
            language_confidence = "low"
        else:
            language_confidence = "medium"

        # Determine structural confidence from consistency score
        if consistency_score > 0.7:
            structural_confidence = "high"
        elif consistency_score > 0.5:
            structural_confidence = "medium"
        else:
            structural_confidence = "low"

        # Check calibration
        calibrated = (language_confidence == structural_confidence)

        # Severity
        severity = "none"
        if not calibrated:
            # High language confidence but low structural = concerning
            if language_confidence == "high" and structural_confidence == "low":
                severity = "high"
            else:
                severity = "medium"

        return {
            "calibrated": calibrated,
            "language_confidence": language_confidence,
            "structural_confidence": structural_confidence,
            "severity": severity
        }

    def check(self, thinking: str, verdict: str,
              generations: Optional[List[Dict]] = None) -> QualityCheck:
        """
        Run all quality checks

        Args:
            thinking: CoT reasoning trace
            verdict: Decision output
            generations: Optional list of multiple generations for consistency check

        Returns:
            QualityCheck object with all results
        """
        # Layer 1: Hallucination
        hallucination = self.detect_hallucination(thinking)

        # Layer 2: Alignment
        alignment = self.check_decision_alignment(thinking, verdict)

        # Layer 3: Consistency (if generations provided)
        if generations:
            consistency = self.check_structural_consistency(generations)
        else:
            consistency = {
                "consistent": True,
                "consistency_score": 1.0,
                "severity": "none"
            }

        # Layer 4: Calibration
        calibration = self.check_confidence_calibration(
            thinking,
            consistency["consistency_score"]
        )

        # Aggregate severity
        severities = [
            hallucination["severity"],
            alignment["severity"],
            consistency["severity"],
            calibration["severity"]
        ]

        # Overall severity is worst of all
        severity_levels = {"none": 0, "medium": 1, "high": 2, "critical": 3}
        max_severity = max(severities, key=lambda s: severity_levels[s])

        # Generate alerts
        alerts = []
        if hallucination["hallucination_detected"]:
            alerts.append(f"Policy hallucination: {hallucination['hallucinated_refs']}")
        if not alignment["aligned"]:
            alerts.append(f"Decision misalignment: {alignment['reason']}")
        if not consistency["consistent"]:
            alerts.append(f"Low consistency: {consistency['consistency_score']:.2f}")
        if not calibration["calibrated"]:
            alerts.append(f"Confidence miscalibration: {calibration['language_confidence']} language vs {calibration['structural_confidence']} structural")

        return QualityCheck(
            hallucination_detected=hallucination["hallucination_detected"],
            hallucinated_refs=hallucination["hallucinated_refs"],
            decision_aligned=alignment["aligned"],
            alignment_reason=alignment["reason"],
            structural_consistency=consistency["consistency_score"],
            consistent=consistency["consistent"],
            confidence_calibrated=calibration["calibrated"],
            language_confidence=calibration["language_confidence"],
            structural_confidence=calibration["structural_confidence"],
            overall_severity=max_severity,
            alerts=alerts
        )

    def log_inference(self, prompt: str, thinking: str, verdict: str,
                     ground_truth: Optional[bool] = None,
                     generations: Optional[List[Dict]] = None,
                     metadata: Optional[Dict] = None) -> Dict:
        """
        Log full inference with quality checks

        Returns complete log entry as dict
        """
        # Run quality checks
        quality = self.check(thinking, verdict, generations)

        # Build log entry
        log_entry = {
            "input": {
                "prompt": prompt,
                "metadata": metadata or {}
            },
            "output": {
                "verdict": verdict,
                "thinking": thinking
            },
            "quality_checks": asdict(quality),
            "ground_truth": {
                "label": ground_truth,
                "correct": (verdict.upper() == "TOXIC") == ground_truth if ground_truth is not None else None
            },
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }

        return log_entry


# Example usage
if __name__ == "__main__":
    POLICY = """
    ## VIOLATES POLICY (TOXIC)
    - T1: Harassment, threats, bullying
    - T2: Sexually explicit content
    - T3: Hate speech

    ## SAFE CONTENT
    - S1: Legitimate questions
    - S2: Educational discussion
    """

    monitor = CoTMonitor(policy=POLICY)

    # Test case 1: Good reasoning
    result1 = monitor.check(
        thinking="This contains harassment (T1) because it insults the person. Verdict: TOXIC",
        verdict="TOXIC"
    )
    print("Test 1 (Good reasoning):")
    print(f"  Hallucination: {result1.hallucination_detected}")
    print(f"  Aligned: {result1.decision_aligned}")
    print(f"  Severity: {result1.overall_severity}")
    print(f"  Alerts: {result1.alerts}")
    print()

    # Test case 2: Hallucinated policy
    result2 = monitor.check(
        thinking="This violates T6 (microaggressions) which is clearly harmful. Verdict: TOXIC",
        verdict="TOXIC"
    )
    print("Test 2 (Hallucinated T6):")
    print(f"  Hallucination: {result2.hallucination_detected}")
    print(f"  Hallucinated refs: {result2.hallucinated_refs}")
    print(f"  Severity: {result2.overall_severity}")
    print(f"  Alerts: {result2.alerts}")
    print()

    # Test case 3: Contradiction
    result3 = monitor.check(
        thinking="This is a respectful educational question (S1). Verdict: TOXIC",
        verdict="TOXIC"
    )
    print("Test 3 (Contradiction):")
    print(f"  Aligned: {result3.decision_aligned}")
    print(f"  Reason: {result3.alignment_reason}")
    print(f"  Severity: {result3.overall_severity}")
    print(f"  Alerts: {result3.alerts}")
    print()

    # Test case 4: Low consistency
    generations = [
        {"thinking": "This violates T1 (harassment). Verdict: TOXIC", "verdict": "TOXIC"},
        {"thinking": "Educational content (S2). Verdict: SAFE", "verdict": "SAFE"},
        {"thinking": "Hate speech detected (T3). Verdict: TOXIC", "verdict": "TOXIC"}
    ]
    result4 = monitor.check(
        thinking=generations[0]["thinking"],
        verdict=generations[0]["verdict"],
        generations=generations
    )
    print("Test 4 (Inconsistent generations):")
    print(f"  Consistent: {result4.consistent}")
    print(f"  Consistency score: {result4.structural_consistency:.2f}")
    print(f"  Severity: {result4.overall_severity}")
    print(f"  Alerts: {result4.alerts}")
